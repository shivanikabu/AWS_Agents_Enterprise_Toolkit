import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import time
import plotly.graph_objects as go
from backend import AgentBackend

# Page configuration
st.set_page_config(page_title="Agent Analytics Dashboard", layout="wide")

# Custom CSS for fonts and styling
st.markdown("""
    <style>
    /* Sidebar configuration title - same size as subheaders */
    [data-testid="stSidebar"] > div:first-child h1 {
        font-size: 24px !important;
        font-weight: 600 !important;
    }
    
    /* Increase sidebar section headers font size */
    .css-1d391kg, [data-testid="stSidebar"] h2, [data-testid="stSidebar"] h3 {
        font-size: 24px !important;
        font-weight: 600 !important;
    }
    
    /* Increase sidebar labels font size */
    [data-testid="stSidebar"] label {
        font-size: 18px !important;
        font-weight: 500 !important;
    }
    
    /* Increase main tab labels font size */
    .stTabs [data-baseweb="tab-list"] button [data-testid="stMarkdownContainer"] p {
        font-size: 24px !important;
        font-weight: 600 !important;
    }
    
    /* Increase tab button text */
    .stTabs [data-baseweb="tab-list"] button {
        font-size: 24px !important;
    }
    
    /* Increase main content headers */
    h1 {
        font-size: 42px !important;
    }
    
    h2 {
        font-size: 32px !important;
    }
    
    h3 {
        font-size: 24px !important;
    }
    
    /* Main title styling */
    .main-title {
        font-size: 48px !important;
        font-weight: 700 !important;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        margin-bottom: 30px !important;
        padding: 20px 0;
    }
    </style>
""", unsafe_allow_html=True)

# Initialize session state
if 'backend' not in st.session_state:
    st.session_state.backend = AgentBackend()
if 'aws_connected' not in st.session_state:
    st.session_state.aws_connected = False
if 'langfuse_connected' not in st.session_state:
    st.session_state.langfuse_connected = False
if 'agent_logs' not in st.session_state:
    st.session_state.agent_logs = []
if 'conversations' not in st.session_state:
    st.session_state.conversations = []

# Main Title
st.markdown('<h1 class="main-title">AI Agents Enterprise Toolkit</h1>', unsafe_allow_html=True)

# Sidebar Configuration Panel
st.sidebar.title("ğŸ”§ Configuration")

# AWS Connectivity
st.sidebar.subheader("AWS API Connectivity")
aws_access_key = st.sidebar.text_input("AWS Access Key", type="password")
aws_secret_key = st.sidebar.text_input("AWS Secret Key", type="password")
aws_region = st.sidebar.text_input("AWS Region", value="us-east-1")

if st.sidebar.button("Connect to AWS"):
    success, message = st.session_state.backend.connect_aws(aws_access_key, aws_secret_key, aws_region)
    if success:
        st.session_state.aws_connected = True
        st.sidebar.success(f"âœ… {message}")
    else:
        st.sidebar.error(f"âŒ {message}")
        st.sidebar.info("ğŸ’¡ Tip: Make sure you have AWS Bedrock access enabled in your account")

# Langfuse Connectivity
st.sidebar.subheader("Langfuse Connectivity")
langfuse_public_key = st.sidebar.text_input("Langfuse Public Key", type="password")
langfuse_secret_key = st.sidebar.text_input("Langfuse Secret Key", type="password")
langfuse_host = st.sidebar.text_input("Langfuse Host", value="https://cloud.langfuse.com")

if st.sidebar.button("Connect to Langfuse"):
    success, message = st.session_state.backend.connect_langfuse(langfuse_public_key, langfuse_secret_key, langfuse_host)
    if success:
        st.session_state.langfuse_connected = True
        st.sidebar.success(f"âœ… {message}")
    else:
        st.sidebar.error(f"âŒ {message}")
        st.sidebar.info("ğŸ’¡ Langfuse is optional. You can proceed without it.")

# Agent Goal Assessment
st.sidebar.subheader("Agent Goal Assessment")
st.sidebar.markdown("**Adjust Weights:**")
risk_weight = st.sidebar.slider("Risk Tolerance", 0.0, 1.0, 0.5, 0.1)
accuracy_weight = st.sidebar.slider("Accuracy", 0.0, 1.0, 0.8, 0.1)
latency_weight = st.sidebar.slider("Latency", 0.0, 1.0, 0.6, 0.1)
cost_weight = st.sidebar.slider("Cost", 0.0, 1.0, 0.4, 0.1)

# Document Upload
st.sidebar.subheader("Document Upload")

# Show current database status
if st.session_state.backend.vector_db is not None:
    try:
        db_contents = st.session_state.backend.vector_db.get()
        if db_contents and 'documents' in db_contents and db_contents['documents']:
            num_docs = len(db_contents['documents'])
            st.sidebar.info(f"ğŸ“š Database: {num_docs} chunks stored")
        else:
            st.sidebar.warning("ğŸ“š Database: Empty - upload documents below")
    except:
        st.sidebar.warning("ğŸ“š Database: Status unknown")
else:
    st.sidebar.warning("ğŸ“š Database: Not initialized")

uploaded_files = st.sidebar.file_uploader(
    "Upload PDF Documents", 
    accept_multiple_files=True,
    type=['pdf'],
    help="Only PDF files are supported"
)

# Guardrails
st.sidebar.subheader("Guardrails")
guardrails = st.sidebar.text_area(
    "Enter Guardrails (one per line)",
    placeholder="No profanity\nNo PII disclosure\nFactual responses only"
)

# Process Documents
if uploaded_files and st.sidebar.button("Process Documents"):
    if not st.session_state.aws_connected:
        st.sidebar.error("âŒ Please connect to AWS first")
    else:
        with st.spinner("Processing documents..."):
            # Use the backend instance from session state
            success, message = st.session_state.backend.process_documents(uploaded_files)
            if success:
                st.sidebar.success(f"âœ… {message}")
                # Force a rerun to ensure state is updated
                st.rerun()
            else:
                st.sidebar.error(f"âŒ {message}")

# Debug: Test document retrieval
if st.sidebar.button("ğŸ” Test Document Retrieval"):
    if st.session_state.backend.vector_db is not None:
        try:
            test_result = st.session_state.backend.vector_db.get()
            if test_result and 'documents' in test_result:
                docs = test_result['documents']
                st.sidebar.success(f"âœ… Found {len(docs)} documents in database")
                if docs:
                    st.sidebar.text(f"First doc preview:\n{docs[0][:100]}...")
            else:
                st.sidebar.error("âŒ No documents found")
        except Exception as e:
            st.sidebar.error(f"âŒ Error: {str(e)}")
    else:
        st.sidebar.error("âŒ Database not initialized")

# Clear database button
if st.sidebar.button("ğŸ—‘ï¸ Clear Vector Database"):
    if st.session_state.backend.vector_db is not None:
        success, message = st.session_state.backend.clear_vector_db()
        if success:
            st.sidebar.success(f"âœ… {message}")
            st.rerun()
        else:
            st.sidebar.error(f"âŒ {message}")
    else:
        st.sidebar.warning("âš ï¸ Database not initialized yet")

# Main Panel - Tabs
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“„ Agent Flow", "ğŸ’¬ Conversations", "ğŸ“Š Agent Analytics", "ğŸ¯ Trajectory Analysis"])

# Tab 1: Agent Flow
with tab1:
    st.header("Agent Flow Visualization")
    
    query_input = st.text_input("Enter your query:", key="query_input")
    
    if st.button("Execute Query"):
        if not query_input:
            st.error("Please enter a query")
        elif not st.session_state.aws_connected:
            st.warning("âš ï¸ AWS not connected. Please connect to AWS in the sidebar to use real AI agents.")
        else:
            st.info("ğŸ¤– Running in REAL MODE with AWS Bedrock AI agents...")
            
            try:
                # Execute the agentic flow using backend
                result = st.session_state.backend.execute_agentic_flow(
                    query_input, 
                    risk_weight, 
                    accuracy_weight, 
                    latency_weight, 
                    cost_weight,
                    guardrails
                )
                
                agents_flow = result["agents_executed"]
                final_response = result["final_response"]
                detected_persona = result["persona"]
                
                # Display agent flow
                for idx, agent in enumerate(agents_flow):
                    # Create colored container based on agent type
                    if "Planner" in agent['agent']:
                        bg_color = "#E3F2FD"
                    elif "Orchestration" in agent['agent']:
                        bg_color = "#F3E5F5"
                    elif "RAG" in agent['agent']:
                        bg_color = "#FFF9C4"
                    elif "Emotions" in agent['agent'] or "Calming" in agent['agent']:
                        bg_color = "#FFE0B2"
                    elif "Best Practices" in agent['agent']:
                        bg_color = "#E1F5FE"
                    elif "Reflector" in agent['agent']:
                        bg_color = "#F8BBD0"
                    elif "Response" in agent['agent']:
                        bg_color = "#C8E6C9"
                    else:
                        bg_color = "#D1C4E9"
                    
                    st.markdown(f"""
                        <div style="background-color: {bg_color}; padding: 15px; border-radius: 10px; margin-bottom: 15px; border-left: 5px solid #1976D2;">
                            <h3 style="margin: 0 0 10px 0;">{agent['emoji']} {agent['agent']} â€¢ {datetime.now().strftime('%H:%M:%S')}</h3>
                            <p style="margin: 5px 0; font-weight: 600;">Action: {agent['action']}</p>
                            <p style="margin: 5px 0; font-size: 14px; color: #424242;">{agent['detail']}</p>
                        </div>
                    """, unsafe_allow_html=True)
                    
                    # Performance check - use actual analysis from Reflector Agent if available
                    if agent['agent'] == "Reflector Agent" and 'performance_status' in agent:
                        # Use actual performance analysis
                        status_map = {
                            'optimal': 'ğŸŸ¢',
                            'acceptable': 'ğŸŸ¡',
                            'warning': 'ğŸ”´'
                        }
                        performance = status_map.get(agent.get('performance_status', 'optimal'), 'ğŸŸ¢')
                        
                        if performance == 'ğŸŸ¢':
                            performance_text = 'Performance: Optimal'
                        elif performance == 'ğŸŸ¡':
                            performance_text = 'Performance: Acceptable (feedback taken)'
                        else:
                            performance_text = 'Performance: Warning (anomaly detected)'
                        
                        performance_issue = agent.get('performance_issue')
                    else:
                        # Default random performance for other agents
                        performance = np.random.choice(['ğŸŸ¢', 'ğŸŸ¡', 'ğŸ”´'], p=[0.7, 0.2, 0.1])
                        performance_text = {
                            'ğŸŸ¢': 'Performance: Optimal',
                            'ğŸŸ¡': 'Performance: Acceptable (feedback taken)',
                            'ğŸ”´': 'Performance: Warning (anomaly detected)'
                        }[performance]
                        
                        # Generate issue summaries for non-Reflector agents
                        issue_summaries = {
                            'ğŸŸ¡': [
                                "Response latency exceeded target threshold by 15%. Optimization recommended.",
                                "Processing time higher than baseline. Consider caching frequently accessed data.",
                                "Agent iteration count above average. Potential optimization opportunity identified."
                            ],
                            'ğŸ”´': [
                                "Critical: Agent timeout occurred. Processing exceeded maximum time limit.",
                                "Critical: Unexpected error in agent execution. Fallback mechanism activated.",
                                "Critical: Resource constraint detected. Agent performance severely degraded."
                            ]
                        }
                        performance_issue = np.random.choice(issue_summaries[performance]) if performance in ['ğŸŸ¡', 'ğŸ”´'] else None
                    
                    col1, col2 = st.columns([1, 4])
                    with col1:
                        st.markdown(f"<h2 style='text-align: center;'>{performance}</h2>", unsafe_allow_html=True)
                    with col2:
                        st.markdown(f"<p style='font-size: 16px; margin-top: 10px;'>{performance_text}</p>", unsafe_allow_html=True)
                        
                        # Show summary for Amber or Red performance
                        if performance in ['ğŸŸ¡', 'ğŸ”´'] and performance_issue:
                            bg_color = "#FFF9C4" if performance == 'ğŸŸ¡' else "#FFCDD2"
                            border_color = '#FFA726' if performance == 'ğŸŸ¡' else '#E53935'
                            st.markdown(f"""
                                <div style="background-color: {bg_color}; padding: 10px; border-radius: 5px; margin-top: 8px; border-left: 3px solid {border_color};">
                                    <p style="margin: 0; font-size: 14px; color: #424242;"><strong>Issue Summary:</strong> {performance_issue}</p>
                                </div>
                            """, unsafe_allow_html=True)
                    
                    time.sleep(0.4)
                    
                    # Log agent activity
                    st.session_state.agent_logs.append({
                        "timestamp": datetime.now(),
                        "agent": agent['agent'],
                        "action": agent['action'],
                        "detail": agent['detail'],
                        "tokens": np.random.randint(100, 1000),
                        "latency": np.random.uniform(0.5, 3.0)
                    })
                
                # Store conversation
                st.session_state.conversations.append({
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "query": query_input,
                    "response": final_response,
                    "persona": detected_persona,
                    "agent_flow": [a['agent'] for a in agents_flow],
                    "mode": "REAL"
                })
                
                st.success("âœ… Query execution completed!")
                
            except Exception as e:
                st.error(f"âŒ Error in agent execution: {str(e)}")
                import traceback
                st.code(traceback.format_exc())

# Tab 2: Conversations
with tab2:
    st.header("Conversations")
    
    if st.session_state.conversations:
        for idx, conv in enumerate(reversed(st.session_state.conversations)):
            mode_badge = "ğŸ¤– REAL MODE" if conv.get('mode') == "REAL" else "ğŸ­ DEMO MODE"
            with st.expander(f"ğŸ” Conversation {len(st.session_state.conversations) - idx} - {conv['timestamp']} | {mode_badge}", expanded=(idx==0)):
                
                # Query section
                st.markdown("### ğŸ™‹ Query")
                st.markdown(f"""
                    <div style="background-color: #E3F2FD; padding: 15px; border-radius: 8px; border-left: 4px solid #2196F3;">
                        <p style="font-size: 16px; margin: 0;">{conv['query']}</p>
                    </div>
                """, unsafe_allow_html=True)
                
                st.markdown("")
                
                # Persona detected
                st.markdown(f"**ğŸ¯ Detected Persona:** `{conv.get('persona', 'N/A')}`")
                
                st.markdown("")
                
                # Response section
                st.markdown("### ğŸ’¬ Final Response")
                st.markdown(f"""
                    <div style="background-color: #E8F5E9; padding: 15px; border-radius: 8px; border-left: 4px solid #4CAF50;">
                        <p style="font-size: 16px; margin: 0; white-space: pre-wrap;">{conv['response']}</p>
                    </div>
                """, unsafe_allow_html=True)
                
                st.markdown("")
                
                # Agent flow
                st.markdown(f"**ğŸ“„ Agent Flow:** {' â†’ '.join(conv.get('agent_flow', []))}")
    else:
        st.info("No conversations yet. Execute a query in the 'Agent Flow' tab to get started.")

# Tab 3: Agent Analytics
with tab3:
    st.header("Agent Analytics")
    
    # Langfuse Dashboard Link
    if st.session_state.langfuse_connected:
        st.markdown("### ğŸ“Š Advanced Analytics")
        st.markdown(f"""
            <div style="background-color: #E8EAF6; padding: 15px; border-radius: 8px; margin-bottom: 20px;">
                <p style="margin: 0; font-size: 16px;">
                    ğŸ”— <a href="{langfuse_host}" target="_blank" style="font-weight: 600;">Open Langfuse Dashboard</a> 
                    for detailed observability and tracing
                </p>
            </div>
        """, unsafe_allow_html=True)
    else:
        st.info("ğŸ’¡ Connect to Langfuse in the sidebar for advanced observability and detailed analytics")
    
    if st.session_state.agent_logs:
        df = pd.DataFrame(st.session_state.agent_logs)
        
        # Token and Latency Charts
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Tokens Consumed Per Agent")
            tokens_by_agent = df.groupby('agent')['tokens'].sum().sort_values(ascending=False)
            st.bar_chart(tokens_by_agent)
        
        with col2:
            st.subheader("Average Latency Per Agent")
            latency_by_agent = df.groupby('agent')['latency'].mean().sort_values(ascending=False)
            st.bar_chart(latency_by_agent)
        
        st.markdown("---")
        
        # Agent Performance Metrics
        st.subheader("ğŸ“ˆ Agent Performance Metrics")
        
        # Create three columns for metrics
        col1, col2, col3 = st.columns(3)
        
        with col1:
            accuracy_score = np.random.uniform(7.5, 9.5)
            st.metric("Accuracy Score", f"{accuracy_score:.1f}/10", 
                     delta=f"{np.random.uniform(-0.5, 0.5):.1f}")
            
            hallucination_score = np.random.uniform(0.1, 0.3)
            st.metric("Hallucination Score", f"{hallucination_score:.2f}", 
                     delta=f"{np.random.uniform(-0.05, 0.02):.2f}", delta_color="inverse")
            
            consistency_score = np.random.uniform(8.0, 9.5)
            st.metric("Consistency Score", f"{consistency_score:.1f}/10",
                     delta=f"{np.random.uniform(-0.3, 0.5):.1f}")
        
        with col2:
            latency_score = np.random.uniform(7.0, 9.0)
            st.metric("Latency Score", f"{latency_score:.1f}/10",
                     delta=f"{np.random.uniform(-0.4, 0.6):.1f}")
            
            throughput = np.random.uniform(2.5, 5.0)
            st.metric("Throughput", f"{throughput:.1f} req/sec",
                     delta=f"{np.random.uniform(-0.3, 0.5):.1f}")
            
            reliability_rate = np.random.uniform(92, 99)
            st.metric("Reliability Rate", f"{reliability_rate:.1f}%",
                     delta=f"{np.random.uniform(-1, 2):.1f}%")
        
        with col3:
            bias_score = np.random.uniform(0.05, 0.20)
            st.metric("Bias Score", f"{bias_score:.2f}",
                     delta=f"{np.random.uniform(-0.03, 0.02):.2f}", delta_color="inverse")
            
            toxicity_score = np.random.uniform(0.01, 0.10)
            st.metric("Toxicity Score", f"{toxicity_score:.2f}",
                     delta=f"{np.random.uniform(-0.02, 0.01):.2f}", delta_color="inverse")
            
            st.metric("Off-Topic Detection", f"{np.random.randint(0, 3)} detected",
                     delta=f"{np.random.randint(-1, 1)}")
        
        st.markdown("---")
        
        # Security Metrics
        st.subheader("ğŸ”’ Security & Safety Metrics")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            denied_topics = np.random.randint(0, 2)
            st.metric("Denied Topics", f"{denied_topics} blocked",
                     help="Number of requests blocked due to policy violations")
        
        with col2:
            jailbreak_attempts = np.random.randint(0, 1)
            st.metric("Jailbreak Detection", f"{jailbreak_attempts} detected",
                     delta_color="off",
                     help="Number of detected attempts to bypass system constraints")
        
        with col3:
            st.metric("Safety Filter Rate", f"{np.random.uniform(98, 100):.1f}%",
                     help="Percentage of responses that passed safety filters")
        
        st.markdown("---")
        
        # Cost Analysis
        st.subheader("ğŸ’° Cost Analysis")
        total_tokens = df['tokens'].sum()
        cost = total_tokens * 0.05 / 1000
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total Cost", f"${cost:.4f}")
        with col2:
            st.metric("Total Tokens", f"{total_tokens:,}")
        with col3:
            avg_cost_per_query = cost / len(st.session_state.conversations) if st.session_state.conversations else 0
            st.metric("Avg Cost/Query", f"${avg_cost_per_query:.4f}")
    else:
        st.info("No agent logs available yet. Execute a query to see analytics.")

# Tab 4: Trajectory Analysis
with tab4:
    st.header("Trajectory Analysis")
    
    st.subheader("Agent Architecture Trajectories")
    
    # Define trajectories with optimal use cases
    trajectories = {
        "Trajectory 1": {
            "agents": [
                "Planner Agent", "Orchestration Agent", "RAG Agent", 
                "Reflector Agent", "Response Agent", "Feedback Agent"
            ],
            "optimal_for": "simple query",
            "risk_base": 0.2  # Low risk
        },
        "Trajectory 2": {
            "agents": [
                "Planner Agent", "Orchestration Agent", "Emotions Agent", 
                "Calming Agent", "RAG Agent", "Reflector Agent", "Response Agent", "Feedback Agent"
            ],
            "optimal_for": ["angry customer", "confused customer"],
            "risk_base": 0.7  # High risk
        },
        "Trajectory 3": {
            "agents": [
                "Planner Agent", "Orchestration Agent", "RAG Agent", 
                "Best Practices Agent", "Reflector Agent", "Response Agent", "Feedback Agent"
            ],
            "optimal_for": "precision ask",
            "risk_base": 0.5  # Medium risk
        }
    }
    
    # Get actual performance metrics from agent analytics
    if st.session_state.agent_logs and st.session_state.conversations:
        df = pd.DataFrame(st.session_state.agent_logs)
        
        # Calculate metrics based on actual data
        num_conversations = len(st.session_state.conversations)
        avg_latency = df['latency'].mean()
        
        # Simulate metrics based on trajectory characteristics
        # In production, these would be calculated from actual data
        base_completion_rate = 0.95 if num_conversations > 0 else 0.90
        base_consistency = 0.88 if num_conversations > 0 else 0.85
        base_error_rate = 0.05 if num_conversations > 0 else 0.08
        base_recovery_time = 2.5 if num_conversations > 0 else 3.0
        base_persona_sensitivity = 0.82 if num_conversations > 0 else 0.80
        base_coherence = 0.91 if num_conversations > 0 else 0.88
    else:
        # Default values if no analytics available
        base_completion_rate = 0.90
        base_consistency = 0.85
        base_error_rate = 0.08
        base_recovery_time = 3.0
        base_persona_sensitivity = 0.80
        base_coherence = 0.88
    
    # Get latest query persona if available
    current_persona = None
    if st.session_state.conversations:
        current_persona = st.session_state.conversations[-1].get('persona', None)
    
    # Calculate trajectory performance metrics
    trajectory_data = []
    
    for traj_name, traj_info in trajectories.items():
        agents = traj_info["agents"]
        
        # Calculate metrics based on trajectory characteristics
        if traj_name == "Trajectory 1":
            # Simple trajectory - high completion, moderate consistency
            completion_rate = base_completion_rate * 1.05  # Best completion
            consistency_index = base_consistency * 0.95  # Moderate consistency
            error_propagation = base_error_rate * 0.8  # Low error rate
            recovery_time = base_recovery_time * 1.2  # Slower recovery
            persona_sensitivity = base_persona_sensitivity * 0.7  # Low sensitivity
            coherence = base_coherence * 0.9  # Lower coherence
            
        elif traj_name == "Trajectory 2":
            # Emotional trajectory - moderate completion, high sensitivity
            completion_rate = base_completion_rate * 0.92  # Lower completion due to complexity
            consistency_index = base_consistency * 1.1  # High consistency for emotional handling
            error_propagation = base_error_rate * 1.5  # Higher error potential
            recovery_time = base_recovery_time * 0.7  # Fast recovery (critical for emotions)
            persona_sensitivity = base_persona_sensitivity * 1.3  # Highest sensitivity
            coherence = base_coherence * 1.15  # Best coherence for conversations
            
        else:  # Trajectory 3
            # Precision trajectory - high accuracy, best coherence
            completion_rate = base_completion_rate * 0.98  # Good completion
            consistency_index = base_consistency * 1.2  # Highest consistency
            error_propagation = base_error_rate * 1.0  # Standard error rate
            recovery_time = base_recovery_time * 0.9  # Good recovery
            persona_sensitivity = base_persona_sensitivity * 1.0  # Moderate sensitivity
            coherence = base_coherence * 1.2  # Highest coherence for technical
        
        # Normalize to appropriate ranges
        completion_rate = min(100, completion_rate * 100)  # Percentage
        consistency_index = min(1.0, consistency_index)  # 0-1 scale
        error_propagation = max(0, min(15, error_propagation * 100))  # Percentage
        recovery_time = max(0.5, recovery_time)  # Seconds
        persona_sensitivity = min(1.0, persona_sensitivity)  # 0-1 scale
        coherence = min(1.0, coherence)  # 0-1 scale
        
        # Check if this is the optimal path for current persona
        optimal_match = False
        if current_persona:
            if isinstance(traj_info["optimal_for"], list):
                optimal_match = current_persona in traj_info["optimal_for"]
            else:
                optimal_match = current_persona == traj_info["optimal_for"]
        
        trajectory_data.append({
            "Trajectory": traj_name,
            "Agents": " â†’ ".join(agents),
            "Optimal For": traj_info["optimal_for"] if isinstance(traj_info["optimal_for"], str) else " / ".join(traj_info["optimal_for"]),
            "Completion Rate": f"{completion_rate:.1f}%",
            "Consistency Index": f"{consistency_index:.3f}",
            "Error Propagation": f"{error_propagation:.1f}%",
            "Recovery Time": f"{recovery_time:.1f}s",
            "Persona Sensitivity": f"{persona_sensitivity:.3f}",
            "Coherence": f"{coherence:.3f}",
            "Is Optimal": "âœ…" if optimal_match else "",
            # Store numeric values for plotting
            "completion_numeric": completion_rate,
            "consistency_numeric": consistency_index,
            "error_numeric": error_propagation,
            "recovery_numeric": recovery_time,
            "sensitivity_numeric": persona_sensitivity,
            "coherence_numeric": coherence
        })
    
    df_trajectories = pd.DataFrame(trajectory_data)
    
    # Display current query context if available
    if current_persona:
        st.info(f"ğŸ¯ **Current Query Persona:** `{current_persona}` - Analyzing optimal trajectory match")
    
    # Display table with new metrics
    display_columns = ["Trajectory", "Completion Rate", "Consistency Index", 
                      "Error Propagation", "Recovery Time", "Persona Sensitivity", 
                      "Coherence"]
    display_df = df_trajectories[display_columns]
    st.dataframe(display_df, use_container_width=True, height=150)
    
    # Add metric guide
    with st.expander("ğŸ“– Metric Definitions Guide"):
        st.markdown("""
        ### Performance Metrics Explained:
        
        **ğŸ¯ Trajectory Completion Rate**
        - Percentage of runs where the agent chain executes fully without breakdown or fallback
        - Higher is better (>95% is excellent)
        
        **ğŸ“Š Consistency Index**
        - Measures variance in responses (semantic similarity across repeated prompts)
        - Scale: 0-1, where 1 is perfect consistency
        - Important for reliability and user trust
        
        **âš ï¸ Error Propagation Rate**
        - Measures how errors from one agent cascade to downstream ones
        - Critical in planner â†’ RAG â†’ reflector paths
        - Lower is better (<5% is excellent)
        
        **â±ï¸ Recovery Time**
        - Time taken to recover from an interruption or escalation
        - Critical for emotional contexts and error handling
        - Measured in seconds, lower is better
        
        **ğŸ­ Persona Sensitivity**
        - Correlation between persona type (e.g., "angry") and adaptation behaviors
        - Measures tone softening, empathy adjustments
        - Scale: 0-1, higher indicates better emotional intelligence
        
        **ğŸ”— Conversational Coherence**
        - Measures topic retention and context continuity across turns
        - Especially important with multiple reasoning agents
        - Scale: 0-1, where 1 is perfect coherence
        """)
    
    # Visualizations
    st.markdown("---")
    st.subheader("ğŸ“Š Trajectory Performance Metrics Comparison")
    
    if len(trajectory_data) > 0:
        # Create comprehensive chart with new metrics
        st.markdown("#### All Metrics Comparison - All Trajectories")
        
        # Prepare data for plotting
        metrics = ['Completion\nRate', 'Consistency\nIndex', 'Error\nPropagation', 
                  'Recovery\nTime', 'Persona\nSensitivity', 'Conversational\nCoherence']
        
        # Normalize all metrics to 0-100 scale for visualization
        traj1_data = df_trajectories[df_trajectories['Trajectory'] == 'Trajectory 1'].iloc[0]
        traj2_data = df_trajectories[df_trajectories['Trajectory'] == 'Trajectory 2'].iloc[0]
        traj3_data = df_trajectories[df_trajectories['Trajectory'] == 'Trajectory 3'].iloc[0]
        
        traj1_values = [
            traj1_data['completion_numeric'],  # Already in percentage
            traj1_data['consistency_numeric'] * 100,  # Convert to percentage
            100 - traj1_data['error_numeric'],  # Invert for "goodness"
            (1 - traj1_data['recovery_numeric']/5) * 100,  # Normalize and invert
            traj1_data['sensitivity_numeric'] * 100,  # Convert to percentage
            traj1_data['coherence_numeric'] * 100  # Convert to percentage
        ]
        
        traj2_values = [
            traj2_data['completion_numeric'],
            traj2_data['consistency_numeric'] * 100,
            100 - traj2_data['error_numeric'],
            (1 - traj2_data['recovery_numeric']/5) * 100,
            traj2_data['sensitivity_numeric'] * 100,
            traj2_data['coherence_numeric'] * 100
        ]
        
        traj3_values = [
            traj3_data['completion_numeric'],
            traj3_data['consistency_numeric'] * 100,
            100 - traj3_data['error_numeric'],
            (1 - traj3_data['recovery_numeric']/5) * 100,
            traj3_data['sensitivity_numeric'] * 100,
            traj3_data['coherence_numeric'] * 100
        ]
        
        # Create plotly figure
        fig = go.Figure(data=[
            go.Bar(name='Trajectory 1 (Simple)', x=metrics, y=traj1_values, 
                   marker_color='#B4D7E8', text=[f'{v:.1f}' for v in traj1_values], textposition='outside'),
            go.Bar(name='Trajectory 2 (Emotional)', x=metrics, y=traj2_values, 
                   marker_color='#C5E1A5', text=[f'{v:.1f}' for v in traj2_values], textposition='outside'),
            go.Bar(name='Trajectory 3 (Precision)', x=metrics, y=traj3_values, 
                   marker_color='#FFCCBC', text=[f'{v:.1f}' for v in traj3_values], textposition='outside')
        ])
        
        fig.update_layout(
            barmode='group',
            height=500,
            xaxis_title="Performance Metrics",
            yaxis_title="Score (0-100 scale, higher is better)",
            yaxis=dict(range=[0, 110]),  # Set y-axis range
            legend=dict(
                orientation="h",
                yanchor="bottom",
                y=1.02,
                xanchor="right",
                x=1
            ),
            plot_bgcolor='white',
            paper_bgcolor='white',
            showlegend=True
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        st.caption("ğŸ“Œ All scores normalized to 0-100 scale for comparison. Higher values indicate better performance.")
        st.caption("âš¡ Recovery Time and Error Propagation are inverted (lower actual values = higher scores)")
    
    # Performance insights based on metrics
    st.markdown("---")
    st.subheader("ğŸ” Performance Insights")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ† Trajectory Strengths")
        st.markdown("""
        **Trajectory 1 (Simple Query)**
        - âœ… Highest completion rate
        - âœ… Lowest error propagation
        - âœ… Most reliable for straightforward queries
        
        **Trajectory 2 (Emotional Support)**
        - âœ… Best persona sensitivity
        - âœ… Fastest recovery time
        - âœ… Superior conversational coherence
        
        **Trajectory 3 (Technical/Precision)**
        - âœ… Highest consistency index
        - âœ… Best coherence for technical content
        - âœ… Balanced performance across metrics
        """)
    
    with col2:
        st.markdown("#### ğŸ“ˆ Optimization Opportunities")
        st.markdown("""
        **Trajectory 1**
        - ğŸ”„ Improve persona sensitivity for better adaptability
        - ğŸ”„ Enhance conversational coherence
        
        **Trajectory 2**
        - ğŸ”„ Reduce error propagation risk
        - ğŸ”„ Improve completion rate stability
        
        **Trajectory 3**
        - ğŸ”„ Optimize recovery time
        - ğŸ”„ Enhance emotional intelligence
        """)
    
    st.markdown("---")
    
    # Trajectory Guide (keeping as requested)
    st.subheader("ğŸ¯ Trajectory Guide")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("### Trajectory 1")
        st.markdown("**Simple Query Path**")
        st.markdown("""
        - ğŸ“‹ **Use Case**: Basic questions
        - ğŸ¯ **Completion**: 95%+
        - ğŸ”„ **Consistency**: Moderate
        - âš¡ **Recovery**: Slower
        - ğŸ’¡ **Best for**: FAQ-style queries
        """)
    
    with col2:
        st.markdown("### Trajectory 2")
        st.markdown("**Emotional Support Path**")
        st.markdown("""
        - ğŸ“‹ **Use Case**: Upset customers
        - â¤ï¸ **Empathy**: Maximum
        - ğŸ›¡ï¸ **Sensitivity**: Highest
        - âš¡ **Recovery**: Fastest
        - ğŸ’¡ **Best for**: De-escalation
        """)
    
    with col3:
        st.markdown("### Trajectory 3")
        st.markdown("**Technical Precision Path**")
        st.markdown("""
        - ğŸ“‹ **Use Case**: Complex queries
        - ğŸ¯ **Accuracy**: Maximum
        - ğŸ”„ **Consistency**: Highest
        - ğŸ“Š **Coherence**: Best
        - ğŸ’¡ **Best for**: Technical support
        """)

# Footer
st.sidebar.markdown("---")
st.sidebar.caption("Agent Analytics Dashboard v1.0")
st.sidebar.caption("Backend: AWS Bedrock | Vector DB: ChromaDB")